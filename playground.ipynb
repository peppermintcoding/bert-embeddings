{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bert for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\WC5SZ5K\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe244f00ea14223902c626d9b253559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be 11 Tokens for Bert\n",
    "sentence = \"Sphinx of black quartz, judge my vow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13 (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 11\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "# Bert and encode_plus\n",
    "inputs = tokenizer.encode_plus(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "print(f\"Number of layers: {len(hidden_states)} (initial embeddings + 12 BERT layers)\")\n",
    "print(f\"Number of batches: {len(hidden_states[0])}\")\n",
    "print(f\"Number of tokens: {len(hidden_states[0][0])}\")\n",
    "print(f\"Number of hidden units: {len(hidden_states[0][0][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = hidden_states[layer_i][0][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 13, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up hidden states\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "# switch to tokens, layers, features\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 11 x 768\n"
     ]
    }
   ],
   "source": [
    "# create a single feature vector per token\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print(f\"Shape is: {len(token_vecs_sum)} x {len(token_vecs_sum[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 27311,  1997,  2304, 20971,  1010,  3648,  2026, 19076,  1012,\n",
       "           102]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sphinx': [27311],\n",
       " 'of': [1997],\n",
       " 'black': [2304],\n",
       " 'quartz,': [20971, 1010],\n",
       " 'judge': [3648],\n",
       " 'my': [2026],\n",
       " 'vow.': [19076, 1012]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: tokenizer.encode(x, add_special_tokens=False) for x in sentence.split()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all.MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus = \"\"\"So she was considering in her own mind (as well as she could,\n",
    "            for the hot day made her feel very sleepy and stupid), whether\n",
    "            the pleasure of making a daisy-chain would be worth the trouble\n",
    "            of getting up and picking the daisies, when suddenly a White\n",
    "            Rabbit with pink eyes ran close by her.\"\"\".split()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(3, len(corpus)):\n",
    "\n",
    "    corpus_embeddings = embedder.encode(corpus[:i], convert_to_tensor=True)\n",
    "    three_dim = PCA(n_components=3, random_state=0).fit_transform(corpus_embeddings)\n",
    "\n",
    "    df = pd.DataFrame(data=[x for x in three_dim], columns=[\"x\", \"y\", \"z\"])\n",
    "    df[\"label\"] = corpus[:i]\n",
    "    df[\"frame\"] = i\n",
    "    df = pd.concat([df, df])\n",
    "\n",
    "# normalize\n",
    "df[\"x\"] = (df[\"x\"] - df[\"x\"].min()) / (df[\"x\"].max() - df[\"x\"].min())\n",
    "df[\"y\"] = (df[\"y\"] - df[\"y\"].min()) / (df[\"y\"].max() - df[\"y\"].min())\n",
    "df[\"z\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import pandas as pd\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_pca_colors(corpus: list, normalize: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Converts a list of words or sentences to color vectors\n",
    "    via PCA dimensionality reduction. Uses the all-MiniLM-l6-v2\n",
    "    Model for embeddings.\n",
    "    \n",
    "    Params:\n",
    "        corpus: a list of strings, either words or sentences\n",
    "        normalize: optional, will normalize the values to 0-1\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: red, green, blue, label\n",
    "    \n",
    "    NOTE: PCA and KernelPCA (with non linear Kernel) give\n",
    "    equal results.\n",
    "    NOTE: TSNE was excluded here because of the higher\n",
    "    computational cost in high dimensions > 50\n",
    "    \"\"\"\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    three_dim = KernelPCA(n_components=3, random_state=0, kernel=\"poly\").fit_transform(corpus_embeddings)\n",
    "\n",
    "    df = pd.DataFrame(data=[x for x in three_dim], columns=[\"red\", \"green\", \"blue\"])\n",
    "    df[\"label\"] = corpus\n",
    "\n",
    "    if normalize:\n",
    "        df[\"red\"] = (df[\"red\"] - df[\"red\"].min()) / (df[\"red\"].max() - df[\"red\"].min())\n",
    "        df[\"green\"] = (df[\"green\"] - df[\"green\"].min()) / (df[\"green\"].max() - df[\"green\"].min())\n",
    "        df[\"blue\"] = (df[\"blue\"] - df[\"blue\"].min()) / (df[\"blue\"].max() - df[\"blue\"].min())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_interpolated_colors(num_colors: int, c1: np.ndarray, c2: np.ndarray) -> list:\n",
    "    \"\"\"Interpolates linearly between two colors\n",
    "    and returns a list of the color gradient c1 -- num_colors -- c2\n",
    "    \"\"\"\n",
    "    segment_len = np.linalg.norm(c2 - c1) / (num_colors+1)\n",
    "    t = (c2 - c1) / np.linalg.norm(c2 - c1)\n",
    "    return [c1 + t*segment_len*i for i in range(num_colors+2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import PIL.ImageFont\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "\n",
    "def save_as_gif(colors: pd.DataFrame, filepath: str, size: tuple = (300, 300), freq: int = 5,\n",
    "                font_size: int = 25, interpolate: bool = False):\n",
    "    \"\"\"Creates and saves a gif from the word to color DataFrame\n",
    "    \n",
    "    Params:\n",
    "        colors: DataFrame with red, green, blue, label\n",
    "        filepath: destination for gif\n",
    "        size: a tuple with (width, height)\n",
    "        freq: repeat rate of frames per color, higher value -> longer per color\n",
    "        font_size: size of the text\n",
    "        interpolate: optional, linearly interpolates between different colors for smoother transition\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    fnt = PIL.ImageFont.truetype(\"FreeMono.ttf\", font_size)\n",
    "    for i, row in colors.iterrows():\n",
    "        if interpolate:\n",
    "            c1 = np.array([row[\"red\"], row[\"green\"], row[\"blue\"]])\n",
    "            if i < colors.shape[0]-1:\n",
    "                row_2 = colors.iloc[i+1, :]\n",
    "                c2 = np.array([row_2[\"red\"], row_2[\"green\"], row_2[\"blue\"]])\n",
    "            else:\n",
    "                c2 = c1.copy()\n",
    "            # leave out the last element because it is the next color in the cycle\n",
    "            gradient = get_interpolated_colors(freq-1, c1, c2)[:-1]\n",
    "            for c in gradient:\n",
    "                img = PIL.Image.new(\"RGB\", size, tuple((c*255).astype(int)))\n",
    "                ctx = PIL.ImageDraw.Draw(img)\n",
    "                label = \" \".join(row[\"label\"])\n",
    "                ctx.text((50, 100), label, fill=(255, 255, 255), font=fnt, stroke_width=1)\n",
    "                images.append(img)\n",
    "        else:\n",
    "            img = PIL.Image.new(\"RGB\", size, (int(row[\"red\"]*255), int(row[\"green\"]*255), int(row[\"blue\"]*255)))\n",
    "            ctx = PIL.ImageDraw.Draw(img)\n",
    "            label = \" \".join(row[\"label\"])\n",
    "            ctx.text((50, 100), label, fill=(255, 255, 255), font=fnt, stroke_width=1)\n",
    "            for _ in range(freq):\n",
    "                images.append(img)\n",
    "    imageio.mimsave(filepath, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_corpus(corpus: str, wl: int) -> list:\n",
    "    \"\"\"Creates a list of sentence fragments\n",
    "    \n",
    "    Params:\n",
    "        corpus: the text as one string\n",
    "        wl: the window length of the sliding window (in words)\n",
    "    \n",
    "    Return:\n",
    "        A list of strings with word length wl\n",
    "    \"\"\"\n",
    "    words = corpus.split()\n",
    "    return [words[i:i+wl] for i in range(len(words)-wl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(s: str) -> str:\n",
    "    \"\"\"Removes punctuation from the string\"\"\"\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"So she was considering in her own mind (as well as she could,\n",
    "            for the hot day made her feel very sleepy and stupid), whether\n",
    "            the pleasure of making a daisy-chain would be worth the trouble\n",
    "            of getting up and picking the daisies, when suddenly a White\n",
    "            Rabbit with pink eyes ran close by her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\", \"r\") as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sliding_window_corpus(corpus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = create_pca_colors(corpus.split())\n",
    "save_as_gif(colors, \"test.gif\", interpolate=True, size=(300, 300))\n",
    "from IPython.display import Image\n",
    "Image(\"test.gif\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for linearity of the color space\n",
    "# using PCA or kernel PCA with a POLY KERNEL does not change\n",
    "# the output too much\n",
    "colors = create_pca_colors(corpus)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = colors[\"red\"].to_numpy().reshape(-1, 1)\n",
    "Y = colors[\"blue\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "regressor = LinearRegression().fit(X, Y)\n",
    "r2_score(regressor.predict(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: upper case and lower case makes a difference, as well as punctuation\n",
    "# should we exclude punctuation?\n",
    "colors = create_pca_colors(\"Some Some, some some. some!\".split())\n",
    "save_as_gif(colors, \"test.gif\")\n",
    "from IPython.display import Image\n",
    "Image(\"test.gif\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"])\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[-1]\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
